{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V0_sanityDS_data_processing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcCrgnSGGmhxuGjtSvV2of",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupermarketAutomationAI/data_processing/blob/main/V0_sanityDS_data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lofQ_cinn2GP"
      },
      "source": [
        "# Data Loading, Splitting, and Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnpKb-WKLSfv"
      },
      "source": [
        "This Jupyter notebook contains the data processing for data processing for the sanity checks. The format of this version is as follows:\n",
        "\n",
        "/V0_sanityDS\n",
        "```\n",
        "/train\n",
        "  Banana\n",
        "  .\n",
        "  .\n",
        "  .\n",
        "/val\n",
        "  Banana\n",
        "  .\n",
        "  .\n",
        "  .\n",
        "/test\n",
        "  Banana\n",
        "  .\n",
        "  .\n",
        "  .\n",
        "```\n",
        "Where each of the above subdirectories in train, val, and test are their own class. In this version, there are 3 classes to classify and since this is intended for sanity checking there are only 100 images in each class.\n",
        "\n",
        "Classes: \n",
        "\n",
        "['Bananas', 'Red Delicious Apples','Orange']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq3gSdZKhKcf"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w72qsUlSn9ux",
        "outputId": "4842e7e9-2226-4b3a-fa5b-ca2631b2e2e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3sFb9inopsm"
      },
      "source": [
        "! unzip '/content/drive/My Drive/APS360 Project/Dataset Zip Files/V0_sanityDS.zip' -d '/root/datasets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ4wt7brH4R-"
      },
      "source": [
        "# Clean the data, remove \"._\" from file names\n",
        "import os\n",
        "from glob import glob\n",
        "parent_dir = \"/root/datasets/V0_sanityDS\"\n",
        "classes = ['Bananas', 'Red Delicious Apples','Orange']\n",
        "\n",
        "for cat in classes:\n",
        "    path = parent_dir + '/' + cat\n",
        "    invalid_files = glob(os.path.join(path,\"._*\"))\n",
        "    for file_ in invalid_files:\n",
        "        os.rename(file_, os.path.join(path,file_.split('/')[-1].replace(\"._\",\"\")))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IrjH_sko_bD"
      },
      "source": [
        "## Split the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SriTVEYqWGAR"
      },
      "source": [
        "# Import the necessary libraries\n",
        "import json\n",
        "import math\n",
        "import shutil"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-do21bIZWK2M"
      },
      "source": [
        "# create data directory and move all images into it\n",
        "parent_dir = \"/root/datasets/V0_sanityDS\"\n",
        "os.chdir(parent_dir)\n",
        "category_list = list(filter(lambda x: os.path.isdir(x), os.listdir()))\n",
        "data_dir = parent_dir + '/' + \"data\"\n",
        "os.mkdir(data_dir, 755)\n",
        "for category in category_list:\n",
        "    cat_dir = parent_dir + '/' + category\n",
        "    shutil.move(cat_dir, data_dir)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4iELB8cImGE"
      },
      "source": [
        "train_split = 0.6\n",
        "\n",
        "dataset_dirs= ['train','val','test']\n",
        "for dsdirs in dataset_dirs:\n",
        " path = parent_dir + '/'+ dsdirs\n",
        " os.mkdir( path,755 )\n",
        "\n",
        "for category in category_list: \n",
        "    src_path = parent_dir + '/data/' + category\n",
        "    train_dir = parent_dir + '/train/' + category + '/'\n",
        "    val_dir = parent_dir + '/val/' + category + '/'\n",
        "    test_dir = parent_dir + '/test/' + category + '/'\n",
        "    \n",
        "    os.mkdir(train_dir, 755 )\n",
        "    os.mkdir(val_dir, 755)\n",
        "    os.mkdir(test_dir, 755)\n",
        "\n",
        "    #get files' names list from respective directories\n",
        "    os.chdir(src_path)\n",
        "    files = [f for f in os.listdir() if os.path.isfile(f)]\n",
        "\n",
        "    #get training, testing and validation files count\n",
        "    train_count = math.ceil(train_split*len(files))\n",
        "    valid_count = int((len(files)-train_count)/2)\n",
        "    test_count = valid_count\n",
        "\n",
        "    #get files to segragate for train,test and validation data set\n",
        "    train_data_list = files[0: train_count-1]\n",
        "    valid_data_list = files[train_count:train_count+valid_count-1] \n",
        "    test_data_list = files[train_count+valid_count:]\n",
        "\n",
        "\n",
        "    for train_data in train_data_list:\n",
        "        train_path = src_path + '/' + train_data\n",
        "        shutil.move(train_path,train_dir)\n",
        "\n",
        "    for valid_data in valid_data_list:\n",
        "        valid_path = src_path + '/' + valid_data\n",
        "        shutil.move(valid_path,val_dir)\n",
        "\n",
        "    for test_data in test_data_list:\n",
        "        test_path = src_path + '/' + test_data\n",
        "        shutil.move(test_path,test_dir)\n",
        "\n",
        "    # Move any files that are left behind into the training directory\n",
        "    os.chdir(src_path)\n",
        "    files = [f for f in os.listdir() if os.path.isfile(f)]\n",
        "    for img_left_behind in files:\n",
        "        img_path = src_path + '/' + img_left_behind\n",
        "        shutil.move(img_path, train_dir)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2JbM8qchO3m"
      },
      "source": [
        "## Preprocess data into Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enhl8HtCizZb"
      },
      "source": [
        "# import needed libraries for this section\n",
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxzhzA1Wf9Y7",
        "outputId": "b493ae8f-cbab-4fd0-e184-a2389a0b7cc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# define the locations of the training and validation data\n",
        "train_dir = os.path.join(parent_dir, 'train/')\n",
        "val_dir = os.path.join(parent_dir, 'val/')\n",
        "test_dir = os.path.join(parent_dir, 'test/')\n",
        "\n",
        "# define a list of all classes that the model will be trained with\n",
        "\n",
        "# List different classes: 14 currently\n",
        "classes = ['Multiple Bananas', 'Multiple Fuji Apples', 'Multiple Gala Apples', 'Multiple Golden Delicious Apples', \n",
        "           'Multiple Granny Smith Apples', 'Multiple Oranges', 'Multiple Red Delicious Apples',\n",
        "           'One Banana', 'One Fuji Apple', 'One Gala Apple', 'One Golden Delicious Apple',\n",
        "           'One Granny Smith Apple', 'One Orange', 'One Red Delicious Apple']\n",
        "\n",
        "# three possible transforms, will test each to see which gives better results\n",
        "data_CC_transform = transforms.Compose([transforms.CenterCrop([224,224]), transforms.ToTensor()])\n",
        "\n",
        "# apply the transforms to the data and set up train_data, val_data, and test_data\n",
        "train_data = datasets.ImageFolder(train_dir, transform=data_CC_transform)\n",
        "val_data = datasets.ImageFolder(val_dir, transform=data_CC_transform)\n",
        "test_data = datasets.ImageFolder(test_dir, transform=data_CC_transform)\n",
        "\n",
        "# print the amount of data in both training and validation sets\n",
        "print(\"Amount of training data: \", len(train_data))\n",
        "print(\"Amount of validation data: \", len(val_data))\n",
        "print(\"Amount of test data: \", len(test_data))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amount of training data:  303\n",
            "Amount of validation data:  97\n",
            "Amount of test data:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CBUeZL8jTDp"
      },
      "source": [
        "# Model Building, Training, and Testing"
      ]
    }
  ]
}